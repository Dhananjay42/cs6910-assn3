{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxkRc/MdJmTKehFqcwYUrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhananjay42/cs6910-assn3/blob/main/assn3_nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CS6910: Assignment-3\n",
        "-- Submitted by Dhananjay Balakrishnan, ME19B012"
      ],
      "metadata": {
        "id": "90nAoRsb0UqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup and Loading the Dataset"
      ],
      "metadata": {
        "id": "ixsJcIep0eUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "AF-X9IyU3Kru"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVnNtzrR0pUt",
        "outputId": "7e221256-daeb-4e34-8355-6a899cd77595"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/gdrive/MyDrive/CS6910_A3/aksharantar_sampled/tam/'"
      ],
      "metadata": {
        "id": "QUrlh1ep0bLk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtain_data(dir):\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  with open(dir, 'r') as file:\n",
        "    reader = csv.DictReader(file, fieldnames=['x', 'y'])\n",
        "  \n",
        "    for row in reader:\n",
        "      x.append(row['x'])\n",
        "      y.append(row['y'])\n",
        "    \n",
        "  return x, y"
      ],
      "metadata": {
        "id": "i2Dr1QDU0qGD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = obtain_data(data_dir + 'tam_train.csv')\n",
        "x_test, y_test = obtain_data(data_dir + 'tam_test.csv')\n",
        "x_val, y_val = obtain_data(data_dir + 'tam_valid.csv')"
      ],
      "metadata": {
        "id": "faHyeHRY3lVl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 2):\n",
        "  print(x_train[i], y_train[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ya5oC143xZ8",
        "outputId": "d63541a6-eb16-4c97-f316-6d7548b180c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thottacharya தொட்டாச்சார்ய\n",
            "menmaithaan மென்மைதான்\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_token = 0\n",
        "end_token = 1"
      ],
      "metadata": {
        "id": "0s9Mght5EB7b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_characters = {}"
      ],
      "metadata": {
        "id": "NkOaYBL_-qy2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Language:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.char2index = {}\n",
        "    self.index2char = {0: \"SOS\", 1: \"EOS\", 2: \"unknown\"}\n",
        "    self.n_chars = 3  # Count SOS and EOS\n",
        "    self.max_size = 0\n",
        "\n",
        "  def update_vocab(self, x):\n",
        "    for word in x:\n",
        "      if len(word) > self.max_size:\n",
        "        self.max_size = len(word)\n",
        "\n",
        "      for letter in word:\n",
        "        if letter not in self.char2index.keys():\n",
        "          self.char2index[letter] = self.n_chars\n",
        "          self.index2char[self.n_chars] = letter\n",
        "          self.n_chars = self.n_chars + 1\n",
        "  \n",
        "  def get_index(self, character):\n",
        "    if character in self.char2index.keys():\n",
        "      return self.char2index[character]\n",
        "    else:\n",
        "      return 2\n",
        "  \n",
        "  def get_character(self, index):\n",
        "    if index == 2:\n",
        "      return '$'\n",
        "    else:\n",
        "      return self.index2char[index]"
      ],
      "metadata": {
        "id": "KItsHUzs9zPP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english = Language('eng')\n",
        "tamil = Language('tam')"
      ],
      "metadata": {
        "id": "Dt5eia4S_tLQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english.update_vocab(x_train)\n",
        "tamil.update_vocab(y_train)"
      ],
      "metadata": {
        "id": "OQBYGHJl_2fQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoded_word(language, word):\n",
        "  coded = [language.get_index(letter) for letter in word]\n",
        "  coded.append(end_token)\n",
        "  return coded\n",
        "\n",
        "def get_pairs(lang1, lang2, inputs, targets):\n",
        "  return [(torch.tensor(encoded_word(lang1, x), dtype=torch.long, device=device).view(-1, 1), torch.tensor(encoded_word(lang2, y), dtype=torch.long, device=device).view(-1, 1)) \n",
        "  for (x,y) in zip(inputs,targets)]\n",
        "\n",
        "def decoded_word(language, encoded_word):\n",
        "  characters = [language.get_character(num) for num in encoded_word[:-1]]\n",
        "  decoded = ''.join(characters)\n",
        "  return decoded"
      ],
      "metadata": {
        "id": "yCsWlmkwAlwC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = get_pairs(english, tamil, x_train, y_train)\n",
        "test_data = get_pairs(english, tamil, x_test, y_test)\n",
        "val_data = get_pairs(english, tamil, x_val, y_val)"
      ],
      "metadata": {
        "id": "4VDupcI8m3aS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN without Attention"
      ],
      "metadata": {
        "id": "63vUwSCMWDMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, inp_vocab_size, embedding_size, n_layers, hl_size, cell_type = 'RNN', bidirectional = True):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.vocab_size = inp_vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hl_size = hl_size\n",
        "    self.bidirectional = bidirectional\n",
        "    self.cell_type = cell_type\n",
        "\n",
        "    if cell_type == 'RNN':\n",
        "      self.cell = nn.RNN(self.embedding_size, self.hl_size, num_layers = self.n_layers, bidirectional = self.bidirectional).to(device)\n",
        "    elif cell_type == 'GRU':\n",
        "      self.cell = nn.GRU(self.embedding_size, self.hl_size, num_layers = self.n_layers, bidirectional = self.bidirectional).to(device)\n",
        "    elif cell_type == 'LSTM':\n",
        "      self.cell = nn.LSTM(self.embedding_size, self.hl_size, num_layers = self.n_layers, bidirectional = self.bidirectional).to(device)\n",
        "    else:\n",
        "      print('Wrong Cell Type.')\n",
        "      exit()\n",
        "    self.embedding_layer = nn.Embedding(self.vocab_size, self.embedding_size).to(device)\n",
        "  \n",
        "  def forward(self, input, hidden, c = 0):\n",
        "    embedded = self.embedding_layer(input).view(1, 1, -1)\n",
        "    if self.cell_type != 'LSTM':\n",
        "      output, hidden = self.cell(embedded, hidden)\n",
        "      return output, hidden\n",
        "    else:\n",
        "      output, (hidden, c) = self.cell(embedded, (hidden, c))\n",
        "      return output, hidden, c\n",
        "  \n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hl_size, device = device)"
      ],
      "metadata": {
        "id": "0zU4N_wh70Wc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderVanilla(nn.Module):\n",
        "  def __init__(self, out_vocab_size, embedding_size, n_layers, hl_size, cell_type = 'RNN', bidirectional = True):\n",
        "    super(DecoderVanilla, self).__init__()\n",
        "    self.vocab_size = out_vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hl_size = hl_size\n",
        "    self.linear = nn.Linear(self.hl_size, self.vocab_size).to(device)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "    self.cell_type = cell_type\n",
        "    self.bidirectional = bidirectional\n",
        "\n",
        "    if cell_type == 'RNN':\n",
        "      self.cell = nn.RNN(self.embedding_size, self.hl_size, num_layers = self.n_layers, bidirectional = self.bidirectional).to(device)\n",
        "    elif cell_type == 'GRU':\n",
        "      self.cell = nn.GRU(self.embedding_size, self.hl_size, num_layers = self.n_layers, bidirectional = self.bidirectional).to(device)\n",
        "    elif cell_type == 'LSTM':\n",
        "      self.cell = nn.LSTM(self.embedding_size, self.hl_size, num_layers = self.n_layers, bidirectional = self.bidirectional).to(device)\n",
        "    else:\n",
        "      print('Wrong Cell Type.')\n",
        "      exit()\n",
        "    \n",
        "    self.embedding_layer = nn.Embedding(self.vocab_size, self.embedding_size).to(device)\n",
        "  \n",
        "  def forward(self, input, hidden, c = 0):\n",
        "    embedded = self.embedding_layer(input).view(1, 1, -1)\n",
        "    output = F.relu(embedded)\n",
        "\n",
        "    if self.cell_type != 'LSTM':\n",
        "      output, hidden = self.cell(output, hidden)\n",
        "      output = self.linear(output[0])\n",
        "      output = self.softmax(output)\n",
        "      return output, hidden\n",
        "    else:\n",
        "      output, (hidden, c) = self.cell(output, (hidden, c))\n",
        "      output = self.linear(output[0])\n",
        "      output = self.softmax(output)\n",
        "      return output, hidden, c"
      ],
      "metadata": {
        "id": "FU3vbDoy1cm8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class seq2seq_vanilla():\n",
        "  def __init__(self, inp_language, out_language, embedding_size, n_layers, hl_size, cell_type = 'LSTM', lr = 0.01, teacher_forcing_ratio = 0.5,bidirectional_flag = False):\n",
        "    self.encoder = Encoder(inp_language.n_chars, embedding_size, n_layers, hl_size, cell_type, bidirectional = bidirectional_flag)\n",
        "    self.decoder = DecoderVanilla(out_language.n_chars, embedding_size, n_layers, hl_size, cell_type, bidirectional = bidirectional_flag)\n",
        "    self.lr = lr\n",
        "    self.teacher_forcing = teacher_forcing_ratio\n",
        "    self.max_length = out_language.max_size\n",
        "    self.cell_type = cell_type\n",
        "\n",
        "    self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr=self.lr)\n",
        "    self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr = self.lr)\n",
        "\n",
        "    self.loss_fn = nn.NLLLoss()\n",
        "\n",
        "  def train_step(self, input, target):\n",
        "    encoder_hidden = self.encoder.init_hidden()\n",
        "    encoder_c = self.encoder.init_hidden()\n",
        "\n",
        "    self.encoder_optimizer.zero_grad()\n",
        "    self.decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input.size(0)\n",
        "    target_length = target.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(0, input_length):\n",
        "      if self.cell_type != 'LSTM':\n",
        "        encoder_output, encoder_hidden = self.encoder.forward(input[i], encoder_hidden)\n",
        "      else:\n",
        "        encoder_output, encoder_hidden, encoder_c = self.encoder.forward(input[i], encoder_hidden, encoder_c)\n",
        "    \n",
        "    decoder_input = torch.tensor([[start_token]], device=device)\n",
        "\n",
        "    #decoder_hidden = encoder_hidden.unsqueeze(1)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_c = encoder_c\n",
        "\n",
        "    num = random.random()\n",
        "\n",
        "    if num < self.teacher_forcing:\n",
        "      #here, we use teacher forcing. \n",
        "      for j in range(0, target_length):\n",
        "        if self.cell_type != 'LSTM':\n",
        "          decoder_output, decoder_hidden = self.decoder.forward(decoder_input, decoder_hidden)\n",
        "        else:\n",
        "          decoder_output, decoder_hidden, decoder_c = self.decoder.forward(decoder_input, decoder_hidden, decoder_c)\n",
        "\n",
        "        loss = loss + self.loss_fn(decoder_output, target[j])\n",
        "        decoder_input = target[j]#.unsqueeze(0)\n",
        "\n",
        "    else:\n",
        "      #here, there is no teacher forcing. the predictions themselves are used. \n",
        "      #outputs = []\n",
        "      for j in range(0, target_length):\n",
        "        if self.cell_type != 'LSTM':\n",
        "          decoder_output, decoder_hidden = self.decoder.forward(decoder_input, decoder_hidden)\n",
        "        else:\n",
        "          decoder_output, decoder_hidden, decoder_c = self.decoder.forward(decoder_input, decoder_hidden, decoder_c)\n",
        "\n",
        "        loss = loss + self.loss_fn(decoder_output, target[j])\n",
        "        value, index = decoder_output.topk(1)\n",
        "        decoder_input = index.squeeze().detach()\n",
        "        #outputs.append(decoder_input.item())\n",
        "        if decoder_input.item() == end_token:\n",
        "          break\n",
        "        \n",
        "        #target_mod = [t[0] for t in target.tolist()]\n",
        "\n",
        "      #print(decoded_word(tamil,outputs), decoded_word(tamil,target_mod))\n",
        "            \n",
        "    loss.backward()\n",
        "    self.encoder_optimizer.step()\n",
        "    self.decoder_optimizer.step()\n",
        "\n",
        "    return loss.item()/target_length\n",
        "  \n",
        "  def predict(self, input, target):\n",
        "    with torch.no_grad():\n",
        "      encoder_hidden = self.encoder.init_hidden()\n",
        "      encoder_c = self.encoder.init_hidden()\n",
        "\n",
        "      input_length = input.size(0)\n",
        "      for i in range(0, input_length):\n",
        "        if self.cell_type != 'LSTM':\n",
        "          encoder_output, encoder_hidden = self.encoder.forward(input[i], encoder_hidden)\n",
        "        else:\n",
        "          encoder_output, encoder_hidden, encoder_c = self.encoder.forward(input[i], encoder_hidden, encoder_c)\n",
        "\n",
        "      decoder_input = torch.tensor([[start_token]], device=device)\n",
        "      #decoder_hidden = encoder_hidden.unsqueeze(1)\n",
        "      decoder_hidden = encoder_hidden\n",
        "      decoder_c = encoder_c\n",
        "\n",
        "      outputs = []\n",
        "      for i in range(0, self.max_length):\n",
        "        if self.cell_type != 'LSTM':\n",
        "          decoder_output, decoder_hidden = self.decoder.forward(decoder_input, decoder_hidden)\n",
        "        else:\n",
        "          decoder_output, decoder_hidden, decoder_c = self.decoder.forward(decoder_input, decoder_hidden, decoder_c)\n",
        "\n",
        "        value, index = decoder_output.data.topk(1)\n",
        "        decoder_input = index.squeeze().detach()\n",
        "        outputs.append(decoder_input.item())\n",
        "        if decoder_input.item() == end_token:\n",
        "          break\n",
        "\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  def evaluate(self, data):\n",
        "    correct = 0\n",
        "    count = 0\n",
        "\n",
        "    for pair in data:\n",
        "      input = pair[0]\n",
        "      target = pair[1]\n",
        "      pred = self.predict(input, target)\n",
        "      target = target.tolist()\n",
        "      target = [t[0] for t in target]\n",
        "\n",
        "      if count%500 == 0:\n",
        "        print(decoded_word(tamil,pred), decoded_word(tamil,target))\n",
        "      count = count + 1\n",
        "      \n",
        "      if len(pred) != len(target):\n",
        "        continue\n",
        "      else:\n",
        "        if pred == target:\n",
        "          correct = correct + 1\n",
        "\n",
        "    return correct/len(x_test)    "
      ],
      "metadata": {
        "id": "XtBOmSCGcuka"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = seq2seq_vanilla(inp_language = english, out_language = tamil, embedding_size = 64, n_layers = 2, hl_size = 128)"
      ],
      "metadata": {
        "id": "-UWaDWHPnXPN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 75000"
      ],
      "metadata": {
        "id": "uKHso_WvDWjg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asm1dpLKgJTi",
        "outputId": "bb20f16d-d797-4f07-d172-e79396235305"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_pairs = [random.choice(train_data) for i in range(0, n_iters)]"
      ],
      "metadata": {
        "id": "-_nDKVgTNmnE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = 0\n",
        "for i in range(0, n_iters):\n",
        "  training_pair = training_pairs[i]\n",
        "  x = training_pair[0]\n",
        "  y = training_pair[1]\n",
        "  loss = model.train_step(x, y)\n",
        "  train_loss = train_loss + loss\n",
        "\n",
        "  if (i+1)%5000 == 0:\n",
        "    print('------------------------------------------------')\n",
        "    print('train loss is:', train_loss/5000)\n",
        "    test_acc = model.evaluate(val_data)\n",
        "    print('test accuracy is:', test_acc)\n",
        "    train_loss = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yiauv0nN19Q",
        "outputId": "99c1402d-1f49-4e08-a354-fcc6b5923ed7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------\n",
            "train loss is: 2.7181953461752504\n",
            "புப்பிப்பப்பத்க் நாணயக்குற்றிகளும்\n",
            "புப்பிப்புக் கௌரவத்தை\n",
            "புப்பு போயிங்\n",
            "புப்பு அடிகளை\n",
            "புப்பிக்் பிஸினஸ்\n",
            "புப்பிப்பி்்் கோட்பாடுகளில்\n",
            "புப்புக் கோவலர்\n",
            "பும் தாலி\n",
            "ப்ப் வண\n",
            "test accuracy is: 0.0\n",
            "------------------------------------------------\n",
            "train loss is: 2.489743823318045\n",
            "பிர்்்்்்்்்்்் நாணயக்குற்றிகளும்\n",
            "தாத்தத்த கௌரவத்தை\n",
            "பிர்்் போயிங்\n",
            "பார்ா் அடிகளை\n",
            "பி்்்்்்் பிஸினஸ்\n",
            "பார்கிக்கக்் கோட்பாடுகளில்\n",
            "பா்்்் கோவலர்\n",
            "கா்் தாலி\n",
            "கின் வண\n",
            "test accuracy is: 0.00048828125\n",
            "------------------------------------------------\n",
            "train loss is: 2.355724639434746\n",
            "துருக்ககககககககக்் நாணயக்குற்றிகளும்\n",
            "துுத்தத்ுு கௌரவத்தை\n",
            "கி்்்்் போயிங்\n",
            "கி்்கள அடிகளை\n",
            "கி்்ப்ப்்் பிஸினஸ்\n",
            "குர்கககககககக் கோட்பாடுகளில்\n",
            "காக்கள் கோவலர்\n",
            "கில் தாலி\n",
            "கின் வண\n",
            "test accuracy is: 0.0\n",
            "------------------------------------------------\n",
            "train loss is: 2.1930159980694723\n",
            "மா்்ககககககககககும் நாணயக்குற்றிகளும்\n",
            "குததததததா கௌரவத்தை\n",
            "பிர்கிக் போயிங்\n",
            "விட்கா அடிகளை\n",
            "பிர்பிர்் பிஸினஸ்\n",
            "கட்பட்ககககககக கோட்பாடுகளில்\n",
            "காகாாா கோவலர்\n",
            "டியா தாலி\n",
            "சான் வண\n",
            "test accuracy is: 0.0009765625\n",
            "------------------------------------------------\n",
            "train loss is: 1.993532317423121\n",
            "ஆல்்்ககக்க்்்ும் நாணயக்குற்றிகளும்\n",
            "கருவதத்தத கௌரவத்தை\n",
            "போலிக்க போயிங்\n",
            "திக்கள அடிகளை\n",
            "பெர்சியின் பிஸினஸ்\n",
            "கொட்டிகள்ளதால் கோட்பாடுகளில்\n",
            "கோவார் கோவலர்\n",
            "டட் தாலி\n",
            "ராக் வண\n",
            "test accuracy is: 0.00341796875\n",
            "------------------------------------------------\n",
            "train loss is: 1.7430892963884037\n",
            "நான்புக்கி்்ுு்்் நாணயக்குற்றிகளும்\n",
            "குறுவவதத் கௌரவத்தை\n",
            "பேச்டிக் போயிங்\n",
            "அடிகளை அடிகளை\n",
            "பிரம்ச்ச் பிஸினஸ்\n",
            "கொட்ப்பாள்ளக் கோட்பாடுகளில்\n",
            "கோவேட் கோவலர்\n",
            "டுட் தாலி\n",
            "வேன் வண\n",
            "test accuracy is: 0.018310546875\n",
            "------------------------------------------------\n",
            "train loss is: 1.4625183778536108\n",
            "நானாக்குக்கி்்ும் நாணயக்குற்றிகளும்\n",
            "குறைத்து கௌரவத்தை\n",
            "போயின் போயிங்\n",
            "அடிகளை அடிகளை\n",
            "புஸிச்சன் பிஸினஸ்\n",
            "கூடப்படுள்ளல் கோட்பாடுகளில்\n",
            "கோலவர் கோவலர்\n",
            "டுல் தாலி\n",
            "வென் வண\n",
            "test accuracy is: 0.065673828125\n",
            "------------------------------------------------\n",
            "train loss is: 1.232263721530048\n",
            "நாணைக்கிற்களும்் நாணயக்குற்றிகளும்\n",
            "கூறுவதததை கௌரவத்தை\n",
            "போயின் போயிங்\n",
            "அடிகளை அடிகளை\n",
            "புஷிச்சன் பிஸினஸ்\n",
            "கோட்டப்புளில் கோட்பாடுகளில்\n",
            "கோலவர் கோவலர்\n",
            "லிட் தாலி\n",
            "வென் வண\n",
            "test accuracy is: 0.14697265625\n",
            "------------------------------------------------\n",
            "train loss is: 1.0541858782729732\n",
            "நாாசக்குருக்கிும் நாணயக்குற்றிகளும்\n",
            "கூற்வதததத் கௌரவத்தை\n",
            "போயின் போயிங்\n",
            "அடிகளை அடிகளை\n",
            "புனிச்சன் பிஸினஸ்\n",
            "கூட்டப்புகளி் கோட்பாடுகளில்\n",
            "கோவார் கோவலர்\n",
            "லாடி தாலி\n",
            "வென் வண\n",
            "test accuracy is: 0.194580078125\n",
            "------------------------------------------------\n",
            "train loss is: 0.9001009927707306\n",
            "நானயுக்குக்குும் நாணயக்குற்றிகளும்\n",
            "கொருவதததத் கௌரவத்தை\n",
            "போயிகன் போயிங்\n",
            "அடிகளை அடிகளை\n",
            "புசினெச்ஸ் பிஸினஸ்\n",
            "கோட்டப்புகள்் கோட்பாடுகளில்\n",
            "கோகளர் கோவலர்\n",
            "லடி தாலி\n",
            "வென் வண\n",
            "test accuracy is: 0.217041015625\n",
            "------------------------------------------------\n",
            "train loss is: 0.7812109478705958\n",
            "நாயாக்க்்ுககிிு் நாணயக்குற்றிகளும்\n",
            "கோறவரத்தத கௌரவத்தை\n",
            "பேயிங்கி போயிங்\n",
            "அடிகளை அடிகளை\n",
            "புஷினசிஸ் பிஸினஸ்\n",
            "கூட்பட்டலிகள் கோட்பாடுகளில்\n",
            "கோவலர் கோவலர்\n",
            "டலி தாலி\n",
            "வென் வண\n",
            "test accuracy is: 0.29443359375\n",
            "------------------------------------------------\n",
            "train loss is: 0.7298942524164554\n",
            "நானயக்குடரிகளும் நாணயக்குற்றிகளும்\n",
            "கோரவத்தயய் கௌரவத்தை\n",
            "போயிங் போயிங்\n",
            "அடிகளை அடிகளை\n",
            "புசிசென்ஸ் பிஸினஸ்\n",
            "கூட்பாடுகளில் கோட்பாடுகளில்\n",
            "கோகாரர் கோவலர்\n",
            "டியோ தாலி\n",
            "வென் வண\n",
            "test accuracy is: 0.29541015625\n",
            "------------------------------------------------\n",
            "train loss is: 0.6650093292418638\n",
            "நானயக்குறிகளகுும் நாணயக்குற்றிகளும்\n",
            "கூறவத்தைத கௌரவத்தை\n",
            "போயிங் போயிங்\n",
            "அடிகளை அடிகளை\n",
            "புசினீஸ் பிஸினஸ்\n",
            "கூட்டப்புகளள் கோட்பாடுகளில்\n",
            "கூவார் கோவலர்\n",
            "தில தாலி\n",
            "வென் வண\n",
            "test accuracy is: 0.335205078125\n",
            "------------------------------------------------\n",
            "train loss is: 0.5791102591084526\n",
            "நாயாக்குற்றிகளும் நாணயக்குற்றிகளும்\n",
            "கோரவத்ததய் கௌரவத்தை\n",
            "போயின் போயிங்\n",
            "அடிகளை அடிகளை\n",
            "புசின்சேஸ் பிஸினஸ்\n",
            "கூட்பாடுகளில் கோட்பாடுகளில்\n",
            "கோகவலர் கோவலர்\n",
            "டலி தாலி\n",
            "வென் வண\n",
            "test accuracy is: 0.3623046875\n",
            "------------------------------------------------\n",
            "train loss is: 0.5544906031979627\n",
            "நானயக்குுுகக்கும் நாணயக்குற்றிகளும்\n",
            "கூரவத்தத் கௌரவத்தை\n",
            "போயின் போயிங்\n",
            "அடிகளை அடிகளை\n",
            "புசெனிஸ் பிஸினஸ்\n",
            "கூட்பாடுகளில் கோட்பாடுகளில்\n",
            "கோவலர் கோவலர்\n",
            "தலியா தாலி\n",
            "வென் வண\n",
            "test accuracy is: 0.364990234375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loss = 0\n",
        "# for i in range(0, n_epochs):\n",
        "#   for training_pair in train_data:\n",
        "#     x = training_pair[0]\n",
        "#     y = training_pair[1]\n",
        "#     loss = model.train_step(x, y)\n",
        "#     train_loss = train_loss + loss\n",
        "\n",
        "#     if (i+1)%5000 == 0:\n",
        "#       print('------------------------------------------------------------')\n",
        "#       print('train loss is:', train_loss/5000)\n",
        "#       test_acc = model.evaluate(training_pairs[0:1000])\n",
        "#       print('test accuracy is:', test_acc)\n",
        "#       train_loss = 0"
      ],
      "metadata": {
        "id": "JIp6NEFkgop2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}