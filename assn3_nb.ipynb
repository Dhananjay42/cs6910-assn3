{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtHJide85Yi/qkrP2CkuNf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhananjay42/cs6910-assn3/blob/main/assn3_nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CS6910: Assignment-3\n",
        "-- Submitted by Dhananjay Balakrishnan, ME19B012"
      ],
      "metadata": {
        "id": "90nAoRsb0UqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup and Loading the Dataset"
      ],
      "metadata": {
        "id": "ixsJcIep0eUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "AF-X9IyU3Kru"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVnNtzrR0pUt",
        "outputId": "bc8cccc1-a461-496a-a075-7735f4535fd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/gdrive/MyDrive/CS6910_A3/aksharantar_sampled/tam/'"
      ],
      "metadata": {
        "id": "QUrlh1ep0bLk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtain_data(dir):\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  with open(dir, 'r') as file:\n",
        "    reader = csv.DictReader(file, fieldnames=['x', 'y'])\n",
        "  \n",
        "    for row in reader:\n",
        "      x.append(row['x'])\n",
        "      y.append(row['y'])\n",
        "    \n",
        "  return x, y"
      ],
      "metadata": {
        "id": "i2Dr1QDU0qGD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = obtain_data(data_dir + 'tam_train.csv')\n",
        "x_test, y_test = obtain_data(data_dir + 'tam_test.csv')\n",
        "x_val, y_val = obtain_data(data_dir + 'tam_valid.csv')"
      ],
      "metadata": {
        "id": "faHyeHRY3lVl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 2):\n",
        "  print(x_train[i], y_train[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ya5oC143xZ8",
        "outputId": "be24b73a-b635-469c-eea7-ce7c7e7217b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thottacharya தொட்டாச்சார்ய\n",
            "menmaithaan மென்மைதான்\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_token = 0\n",
        "end_token = 1"
      ],
      "metadata": {
        "id": "0s9Mght5EB7b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_characters = {}"
      ],
      "metadata": {
        "id": "NkOaYBL_-qy2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Language:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.char2index = {}\n",
        "    self.index2char = {0: \"SOS\", 1: \"EOS\", 2: \"unknown\"}\n",
        "    self.n_chars = 3  # Count SOS and EOS\n",
        "    self.max_size = 0\n",
        "\n",
        "  def update_vocab(self, x):\n",
        "    for word in x:\n",
        "      if len(word) > self.max_size:\n",
        "        self.max_size = len(word)\n",
        "        \n",
        "      for letter in word:\n",
        "        if letter not in self.char2index.keys():\n",
        "          self.char2index[letter] = self.n_chars\n",
        "          self.index2char[self.n_chars] = letter\n",
        "          self.n_chars = self.n_chars + 1\n",
        "  \n",
        "  def get_index(self, character):\n",
        "    if character in self.char2index.keys():\n",
        "      return self.char2index[character]\n",
        "    else:\n",
        "      return 2"
      ],
      "metadata": {
        "id": "KItsHUzs9zPP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english = Language('eng')\n",
        "tamil = Language('tam')"
      ],
      "metadata": {
        "id": "Dt5eia4S_tLQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english.update_vocab(x_train)\n",
        "tamil.update_vocab(y_train)"
      ],
      "metadata": {
        "id": "OQBYGHJl_2fQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoded_word(language, word):\n",
        "  coded = [language.get_index(letter) for letter in word]\n",
        "  coded.append(end_token)\n",
        "  return coded\n",
        "\n",
        "def get_pairs(lang1, lang2, inputs, targets):\n",
        "  return [(torch.tensor(encoded_word(lang1, x), dtype=torch.long, device=device).view(-1, 1), torch.tensor(encoded_word(lang2, y), dtype=torch.long, device=device).view(-1, 1)) \n",
        "  for (x,y) in zip(inputs,targets)]"
      ],
      "metadata": {
        "id": "yCsWlmkwAlwC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = get_pairs(english, tamil, x_train, y_train)\n",
        "test_data = get_pairs(english, tamil, x_test, y_test)\n",
        "val_data = get_pairs(english, tamil, x_val, y_val)"
      ],
      "metadata": {
        "id": "4VDupcI8m3aS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN without Attention"
      ],
      "metadata": {
        "id": "63vUwSCMWDMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, inp_vocab_size, embedding_size, n_layers, hl_size, cell_type = 'RNN'):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.vocab_size = inp_vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hl_size = hl_size\n",
        "\n",
        "    if cell_type == 'RNN':\n",
        "      self.cell = nn.RNN(self.embedding_size, self.hl_size, num_layers = self.n_layers).to(device)\n",
        "    elif cell_type == 'GRU':\n",
        "      self.cell = nn.GRU(self.embedding_size, self.hl_size, num_layers = self.n_layers).to(device)\n",
        "    elif cell_type == 'LSTM':\n",
        "      self.cell = nn.LSTM(self.embedding_size, self.hl_size, num_layers = self.n_layers).to(device)\n",
        "      #you'll have to fix the LSTM part as the return/passed parameters are different. \n",
        "    else:\n",
        "      print('Wrong Cell Type.')\n",
        "      exit()\n",
        "    self.embedding_layer = nn.Embedding(self.vocab_size, self.embedding_size).to(device)\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding_layer(input)\n",
        "    output, hidden = self.cell(embedded)\n",
        "    return output, hidden\n",
        "  \n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hl_size, device = device)"
      ],
      "metadata": {
        "id": "0zU4N_wh70Wc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderVanilla(nn.Module):\n",
        "  def __init__(self, out_vocab_size, embedding_size, n_layers, hl_size, cell_type = 'RNN'):\n",
        "    super(DecoderVanilla, self).__init__()\n",
        "    self.vocab_size = out_vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hl_size = hl_size\n",
        "    self.linear = nn.Linear(self.hl_size, self.vocab_size).to(device)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    if cell_type == 'RNN':\n",
        "      self.cell = nn.RNN(self.embedding_size, self.hl_size, num_layers = self.n_layers).to(device)\n",
        "    elif cell_type == 'GRU':\n",
        "      self.cell = nn.GRU(self.embedding_size, self.hl_size, num_layers = self.n_layers).to(device)\n",
        "    elif cell_type == 'LSTM':\n",
        "      self.cell = nn.LSTM(self.embedding_size, self.hl_size, num_layers = self.n_layers).to(device)\n",
        "    else:\n",
        "      print('Wrong Cell Type.')\n",
        "      exit()\n",
        "    \n",
        "    self.embedding_layer = nn.Embedding(self.vocab_size, self.embedding_size).to(device)\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding_layer(input)\n",
        "    output = F.relu(embedded)\n",
        "\n",
        "    output, hidden = self.cell(output, hidden)\n",
        "    output = self.linear(output[0])\n",
        "    output = self.softmax(output)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "FU3vbDoy1cm8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class seq2seq_vanilla():\n",
        "  def __init__(self, inp_language, out_language, embedding_size, n_layers, hl_size, cell_type = 'RNN', lr = 0.001, teacher_forcing_ratio = 0.5):\n",
        "    self.encoder = Encoder(inp_language.n_chars, embedding_size, n_layers, hl_size, cell_type)\n",
        "    self.decoder = DecoderVanilla(out_language.n_chars, embedding_size, n_layers, hl_size, cell_type)\n",
        "    self.lr = lr\n",
        "    self.teacher_forcing = teacher_forcing_ratio\n",
        "    self.max_length = out_language.max_size\n",
        "\n",
        "    self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr=self.lr)\n",
        "    self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr = self.lr)\n",
        "\n",
        "    self.loss_fn = nn.NLLLoss()\n",
        "\n",
        "  def train_step(self, input, target):\n",
        "    encoder_hidden = self.encoder.init_hidden()\n",
        "    self.encoder_optimizer.zero_grad()\n",
        "    self.decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input.size(0)\n",
        "    target_length = target.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(0, input_length):\n",
        "      encoder_output, encoder_hidden = self.encoder.forward(input[i], encoder_hidden)\n",
        "    \n",
        "    decoder_input = torch.tensor([[start_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden.unsqueeze(1)\n",
        "\n",
        "    if random.random() < self.teacher_forcing:\n",
        "      #here, we use teacher forcing. \n",
        "      for j in range(0, target_length):\n",
        "        decoder_output, decoder_hidden = self.decoder.forward(decoder_input, decoder_hidden)\n",
        "        loss = loss + self.loss_fn(decoder_output, target[j])\n",
        "        decoder_input = target[j].unsqueeze(0)\n",
        "\n",
        "    else:\n",
        "      #here, there is no teacher forcing. the predictions themselves are used. \n",
        "      for j in range(0, target_length):\n",
        "        decoder_output, decoder_hidden = self.decoder.forward(decoder_input, decoder_hidden)\n",
        "        loss = loss + self.loss_fn(decoder_output, target[j])\n",
        "        value, index = decoder_output.topk(1)\n",
        "        decoder_input = index\n",
        "        if decoder_input.item() == end_token:\n",
        "          break\n",
        "            \n",
        "    loss.backward()\n",
        "    self.encoder_optimizer.step()\n",
        "    self.decoder_optimizer.step()\n",
        "\n",
        "    return loss.item()/target_length\n",
        "  \n",
        "  def predict(self, input, target):\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "      encoder_hidden = self.encoder.init_hidden()\n",
        "\n",
        "      input_length = input.size(0)\n",
        "      for i in range(0, input_length):\n",
        "        encoder_output, encoder_hidden = self.encoder.forward(input[i], encoder_hidden)\n",
        "\n",
        "      decoder_input = torch.tensor([[start_token]], device=device)\n",
        "      decoder_hidden = encoder_hidden.unsqueeze(1)\n",
        "\n",
        "      outputs = []\n",
        "      for i in range(0, min(target.size(0), self.max_length)):\n",
        "        decoder_output, decoder_hidden = self.decoder.forward(decoder_input, decoder_hidden)\n",
        "        loss = loss + self.loss_fn(decoder_output, target[i])\n",
        "\n",
        "        value, index = decoder_output.topk(1)\n",
        "        decoder_input = index\n",
        "        outputs.append(decoder_input.item())\n",
        "        if decoder_input.item() == end_token:\n",
        "          break\n",
        "\n",
        "      return loss.item()/len(outputs), outputs\n",
        "\n",
        "\n",
        "  def evaluate(self, data, loss_flag = True):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for pair in data:\n",
        "      input = pair[0]\n",
        "      target = pair[1]\n",
        "      loss, pred = self.predict(input, target)\n",
        "      target = target.tolist()\n",
        "      \n",
        "      if loss_flag:\n",
        "        total_loss = total_loss + loss\n",
        "      \n",
        "      if len(pred) != len(target):\n",
        "        continue\n",
        "      else:\n",
        "        if pred == target:\n",
        "          correct = correct + 1\n",
        "    \n",
        "    if loss_flag:\n",
        "      return total_loss, correct/len(x_test)\n",
        "\n",
        "    else:\n",
        "      return correct/len(x_test)    "
      ],
      "metadata": {
        "id": "XtBOmSCGcuka"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = seq2seq_vanilla(inp_language = english, out_language = tamil, embedding_size = 16, n_layers = 3, hl_size = 64)"
      ],
      "metadata": {
        "id": "-UWaDWHPnXPN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 45000"
      ],
      "metadata": {
        "id": "uKHso_WvDWjg"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_pairs = [random.choice(train_data) for i in range(0, n_iters)]"
      ],
      "metadata": {
        "id": "-_nDKVgTNmnE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = 0\n",
        "for i in range(0, n_iters):\n",
        "  training_pair = training_pairs[i]\n",
        "  x = training_pair[0]\n",
        "  y = training_pair[1]\n",
        "  loss = model.train_step(x, y)\n",
        "  train_loss = train_loss + loss\n",
        "\n",
        "  if (i+1)%5000 == 0:\n",
        "     print(train_loss/5000)\n",
        "     test_loss, test_acc = model.evaluate(val_data, loss_flag = True)\n",
        "     print(test_loss, test_acc)\n",
        "     train_loss = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yiauv0nN19Q",
        "outputId": "0665b7b7-9e6d-42de-c518-217a1dcc3c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7873415006148496\n",
            "12384.246377424968 0.0\n",
            "2.593003779720923\n",
            "12680.817673816937 0.0\n",
            "2.5375316890756086\n",
            "12222.915682475545 0.0\n",
            "2.510175545051388\n",
            "12210.768765169512 0.0\n",
            "2.4818409175910765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, n_epochs):\n",
        "  train_loss = 0\n",
        "\n",
        "  for training_pair in train_data:\n",
        "    x = training_pair[0]\n",
        "    y = training_pair[1]\n",
        "    loss = model.train_step(x, y)\n",
        "    train_loss = train_loss + loss\n",
        "  \n",
        "  train_acc = model.evaluate(train_data, loss_flag = False)\n",
        "  test_loss, test_acc = model.evaluate(val_data, loss_flag = True)\n",
        "  print(f'At the end of epoch {i+1}, train loss:{train_loss}, train accuracy:{train_acc}, test loss:{test_loss}, and test accuracy: {test_acc}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "GXTuhVm_KSBg",
        "outputId": "6db4b59b-de86-4e32-cb31-9c1202638a9f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2a86287d353a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-b30aae022db2>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nl4g-P11jkz5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}